{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape tweets for a period of 6 months. This has resulted in approximately 43.7K tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GetOldTweets3 as got\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# NTLK function\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tokenize as tok\n",
    "from nltk.stem.snowball import SnowballStemmer # load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "import lda # topic modeling -NMF & LDA\n",
    "import string\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "# Tf-Idf and Clustering packages\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text clustering with K-means and tf-idf\n",
    "https://medium.com/@MSalnikov/text-clustering-with-k-means-and-tf-idf-f099bcf95183"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = ['mortgage','current account','savings account','insurance','credit card','pension',\n",
    "                'personal loan','money transfer','tax advice','investment','wealth management']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mortgage\n",
      "current account\n",
      "savings account\n",
      "insurance\n",
      "credit card\n",
      "pension\n",
      "personal loan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "money transfer\n",
      "tax advice\n",
      "investment\n",
      "wealth management\n"
     ]
    }
   ],
   "source": [
    "# scrape the data (terms)\n",
    "tweet_df_all = pd.DataFrame()\n",
    "for term in search_terms:\n",
    "    print(term)\n",
    "    tweetCriteria = got.manager.TweetCriteria().setQuerySearch(term)\\\n",
    "                                               .setSince(\"2019-04-13\")\\\n",
    "                                               .setUntil(\"2019-09-28\")\\\n",
    "                                               .setNear(\"Hong Kong\")\\\n",
    "                                               .setWithin(\"310mi\")\n",
    "    tweet = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "    tweet_list = [[tweet[x].id,\n",
    "                  tweet[x].author_id,\n",
    "                  tweet[x].text,\n",
    "                  tweet[x].retweets,\n",
    "                  tweet[x].permalink,\n",
    "                  tweet[x].date,\n",
    "                  tweet[x].formatted_date,\n",
    "                  tweet[x].favorites,\n",
    "                  tweet[x].mentions,\n",
    "                  tweet[x].hashtags,\n",
    "                  tweet[x].geo,\n",
    "                  tweet[x].urls\n",
    "                 ]for x in range(0, len(tweet))]\n",
    "    tweet_df = pd.DataFrame(tweet_list)\n",
    "    tweet_df['search_term'] = term\n",
    "    tweet_df_all = tweet_df_all.append(tweet_df)\n",
    "\n",
    "\n",
    "tweet_df_all.columns = ['id','author_id','text','retweets','permalink','date','formatted_date','favorites','mentions','hashtags','geo','urls','search_term']\n",
    "\n",
    "                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(385, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df_all.to_csv(r\"C:\\Users\\USER\\Desktop\\twitter-sentiment-stock-prediction\\data\\all_tweet.csv\", index=False)\n",
    "tweet_df_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add List of Financial Institutions providng the above products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df_all = pd.read_csv(r'C:\\Users\\USER\\Desktop\\twitter-sentiment-stock-prediction\\data\\all_tweet.csv')\n",
    "tweet_df_all = tweet_df_all[tweet_df_all['text'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(385, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweets</th>\n",
       "      <th>permalink</th>\n",
       "      <th>date</th>\n",
       "      <th>formatted_date</th>\n",
       "      <th>favorites</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>geo</th>\n",
       "      <th>urls</th>\n",
       "      <th>search_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1177593827768422400</td>\n",
       "      <td>8.579987e+08</td>\n",
       "      <td>It is a free market so loans will exist. Howev...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://twitter.com/liujackc/status/1177593827...</td>\n",
       "      <td>2019-09-27 14:40:21+00:00</td>\n",
       "      <td>Fri Sep 27 14:40:21 +0000 2019</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1169176926977581057</td>\n",
       "      <td>2.211808e+07</td>\n",
       "      <td>Lucky bastard! Actually I know someone who did...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://twitter.com/Steve_Dunthorne/status/116...</td>\n",
       "      <td>2019-09-04 09:14:36+00:00</td>\n",
       "      <td>Wed Sep 04 09:14:36 +0000 2019</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1167796354032078850</td>\n",
       "      <td>2.566628e+09</td>\n",
       "      <td>Football historians will look back in astonish...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://twitter.com/LysaghtSteph/status/116779...</td>\n",
       "      <td>2019-08-31 13:48:42+00:00</td>\n",
       "      <td>Sat Aug 31 13:48:42 +0000 2019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1164907704894099462</td>\n",
       "      <td>5.383800e+07</td>\n",
       "      <td>#pirata #bravo #italian Best Italian food with...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://twitter.com/raymoorecn/status/11649077...</td>\n",
       "      <td>2019-08-23 14:30:14+00:00</td>\n",
       "      <td>Fri Aug 23 14:30:14 +0000 2019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#pirata #bravo #italian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.instagram.com/p/B1gojG0gLfRj02mhIX...</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1163704451305054208</td>\n",
       "      <td>2.162408e+09</td>\n",
       "      <td>Most HK Chinese were in that camp in 1997. Hat...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://twitter.com/DrJim_Walker/status/116370...</td>\n",
       "      <td>2019-08-20 06:48:56+00:00</td>\n",
       "      <td>Tue Aug 20 06:48:56 +0000 2019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id     author_id  \\\n",
       "0  1177593827768422400  8.579987e+08   \n",
       "1  1169176926977581057  2.211808e+07   \n",
       "2  1167796354032078850  2.566628e+09   \n",
       "3  1164907704894099462  5.383800e+07   \n",
       "4  1163704451305054208  2.162408e+09   \n",
       "\n",
       "                                                text  retweets  \\\n",
       "0  It is a free market so loans will exist. Howev...       0.0   \n",
       "1  Lucky bastard! Actually I know someone who did...       0.0   \n",
       "2  Football historians will look back in astonish...       0.0   \n",
       "3  #pirata #bravo #italian Best Italian food with...       0.0   \n",
       "4  Most HK Chinese were in that camp in 1997. Hat...       0.0   \n",
       "\n",
       "                                           permalink  \\\n",
       "0  https://twitter.com/liujackc/status/1177593827...   \n",
       "1  https://twitter.com/Steve_Dunthorne/status/116...   \n",
       "2  https://twitter.com/LysaghtSteph/status/116779...   \n",
       "3  https://twitter.com/raymoorecn/status/11649077...   \n",
       "4  https://twitter.com/DrJim_Walker/status/116370...   \n",
       "\n",
       "                        date                  formatted_date  favorites  \\\n",
       "0  2019-09-27 14:40:21+00:00  Fri Sep 27 14:40:21 +0000 2019        3.0   \n",
       "1  2019-09-04 09:14:36+00:00  Wed Sep 04 09:14:36 +0000 2019        2.0   \n",
       "2  2019-08-31 13:48:42+00:00  Sat Aug 31 13:48:42 +0000 2019        0.0   \n",
       "3  2019-08-23 14:30:14+00:00  Fri Aug 23 14:30:14 +0000 2019        0.0   \n",
       "4  2019-08-20 06:48:56+00:00  Tue Aug 20 06:48:56 +0000 2019        0.0   \n",
       "\n",
       "  mentions                 hashtags  geo  \\\n",
       "0      NaN                      NaN  NaN   \n",
       "1      NaN                      NaN  NaN   \n",
       "2      NaN                      NaN  NaN   \n",
       "3      NaN  #pirata #bravo #italian  NaN   \n",
       "4      NaN                      NaN  NaN   \n",
       "\n",
       "                                                urls search_term  \n",
       "0                                                NaN    mortgage  \n",
       "1                                                NaN    mortgage  \n",
       "2                                                NaN    mortgage  \n",
       "3  https://www.instagram.com/p/B1gojG0gLfRj02mhIX...    mortgage  \n",
       "4                                                NaN    mortgage  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tweet_df_all.shape);tweet_df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "search_term\n",
       "credit card           42\n",
       "current account        3\n",
       "insurance             71\n",
       "investment           216\n",
       "money transfer         2\n",
       "mortgage              18\n",
       "pension               25\n",
       "savings account        2\n",
       "tax advice             1\n",
       "wealth management      5\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df_all['text'] = tweet_df_all['text'].str.lower()\n",
    "tweet_df_comp = tweet_df_all\n",
    "tweet_df_comp.groupby('search_term')['id'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Extraction with LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove unnessary words\n",
    "#Complie all regular expressions\n",
    "isURL = re.compile(r'http[s]?:// (?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', re.VERBOSE | re.IGNORECASE)\n",
    "isRTusername = re.compile(r'^RT+[\\s]+(@[\\w_]+:)',re.VERBOSE | re.IGNORECASE) #r'^RT+[\\s]+(@[\\w_]+:)'\n",
    "isEntity = re.compile(r'@[\\w_]+', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "# Helper functions\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]])) \n",
    "\n",
    "# Show top n keywords for each topic\n",
    "def show_topics(vectorizer, lda_model, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "     \n",
    "        \n",
    "def clean_tweet(row):\n",
    "    row = isURL.sub(\"\",row)\n",
    "    row = isRTusername.sub(\"\",row)\n",
    "    row = isEntity.sub(\"\",row)\n",
    "    return row\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in tok.sent_tokenize(text) for word in tok.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweets</th>\n",
       "      <th>permalink</th>\n",
       "      <th>date</th>\n",
       "      <th>formatted_date</th>\n",
       "      <th>favorites</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>geo</th>\n",
       "      <th>urls</th>\n",
       "      <th>search_term</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1177593827768422400</td>\n",
       "      <td>8.579987e+08</td>\n",
       "      <td>it is a free market so loans will exist. howev...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://twitter.com/liujackc/status/1177593827...</td>\n",
       "      <td>2019-09-27 14:40:21+00:00</td>\n",
       "      <td>Fri Sep 27 14:40:21 +0000 2019</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mortgage</td>\n",
       "      <td>it is a free market so loans will exist howeve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1169176926977581057</td>\n",
       "      <td>2.211808e+07</td>\n",
       "      <td>lucky bastard! actually i know someone who did...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://twitter.com/Steve_Dunthorne/status/116...</td>\n",
       "      <td>2019-09-04 09:14:36+00:00</td>\n",
       "      <td>Wed Sep 04 09:14:36 +0000 2019</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mortgage</td>\n",
       "      <td>lucky bastard actually i know someone who did ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1167796354032078850</td>\n",
       "      <td>2.566628e+09</td>\n",
       "      <td>football historians will look back in astonish...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://twitter.com/LysaghtSteph/status/116779...</td>\n",
       "      <td>2019-08-31 13:48:42+00:00</td>\n",
       "      <td>Sat Aug 31 13:48:42 +0000 2019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mortgage</td>\n",
       "      <td>football historians will look back in astonish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1164907704894099462</td>\n",
       "      <td>5.383800e+07</td>\n",
       "      <td>#pirata #bravo #italian best italian food with...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://twitter.com/raymoorecn/status/11649077...</td>\n",
       "      <td>2019-08-23 14:30:14+00:00</td>\n",
       "      <td>Fri Aug 23 14:30:14 +0000 2019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#pirata #bravo #italian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.instagram.com/p/B1gojG0gLfRj02mhIX...</td>\n",
       "      <td>mortgage</td>\n",
       "      <td>pirata bravo italian best italian food without...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1163704451305054208</td>\n",
       "      <td>2.162408e+09</td>\n",
       "      <td>most hk chinese were in that camp in 1997. hat...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://twitter.com/DrJim_Walker/status/116370...</td>\n",
       "      <td>2019-08-20 06:48:56+00:00</td>\n",
       "      <td>Tue Aug 20 06:48:56 +0000 2019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mortgage</td>\n",
       "      <td>most hk chinese were in that camp in 1997 hate...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id     author_id  \\\n",
       "0  1177593827768422400  8.579987e+08   \n",
       "1  1169176926977581057  2.211808e+07   \n",
       "2  1167796354032078850  2.566628e+09   \n",
       "3  1164907704894099462  5.383800e+07   \n",
       "4  1163704451305054208  2.162408e+09   \n",
       "\n",
       "                                                text  retweets  \\\n",
       "0  it is a free market so loans will exist. howev...       0.0   \n",
       "1  lucky bastard! actually i know someone who did...       0.0   \n",
       "2  football historians will look back in astonish...       0.0   \n",
       "3  #pirata #bravo #italian best italian food with...       0.0   \n",
       "4  most hk chinese were in that camp in 1997. hat...       0.0   \n",
       "\n",
       "                                           permalink  \\\n",
       "0  https://twitter.com/liujackc/status/1177593827...   \n",
       "1  https://twitter.com/Steve_Dunthorne/status/116...   \n",
       "2  https://twitter.com/LysaghtSteph/status/116779...   \n",
       "3  https://twitter.com/raymoorecn/status/11649077...   \n",
       "4  https://twitter.com/DrJim_Walker/status/116370...   \n",
       "\n",
       "                        date                  formatted_date  favorites  \\\n",
       "0  2019-09-27 14:40:21+00:00  Fri Sep 27 14:40:21 +0000 2019        3.0   \n",
       "1  2019-09-04 09:14:36+00:00  Wed Sep 04 09:14:36 +0000 2019        2.0   \n",
       "2  2019-08-31 13:48:42+00:00  Sat Aug 31 13:48:42 +0000 2019        0.0   \n",
       "3  2019-08-23 14:30:14+00:00  Fri Aug 23 14:30:14 +0000 2019        0.0   \n",
       "4  2019-08-20 06:48:56+00:00  Tue Aug 20 06:48:56 +0000 2019        0.0   \n",
       "\n",
       "  mentions                 hashtags  geo  \\\n",
       "0      NaN                      NaN  NaN   \n",
       "1      NaN                      NaN  NaN   \n",
       "2      NaN                      NaN  NaN   \n",
       "3      NaN  #pirata #bravo #italian  NaN   \n",
       "4      NaN                      NaN  NaN   \n",
       "\n",
       "                                                urls search_term  \\\n",
       "0                                                NaN    mortgage   \n",
       "1                                                NaN    mortgage   \n",
       "2                                                NaN    mortgage   \n",
       "3  https://www.instagram.com/p/B1gojG0gLfRj02mhIX...    mortgage   \n",
       "4                                                NaN    mortgage   \n",
       "\n",
       "                                          text_clean  \n",
       "0  it is a free market so loans will exist howeve...  \n",
       "1  lucky bastard actually i know someone who did ...  \n",
       "2  football historians will look back in astonish...  \n",
       "3  pirata bravo italian best italian food without...  \n",
       "4  most hk chinese were in that camp in 1997 hate...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df_comp['text_clean'] = tweet_df_comp['text'].apply(lambda row:clean_tweet(row))\n",
    "\n",
    "#remove punctuations\n",
    "RE_PUNCTUATION = '|'.join([re.escape(x) for x in string.punctuation])  \n",
    "tweet_df_comp['text_clean'] = tweet_df_comp['text_clean'].str.replace(RE_PUNCTUATION, \"\")\n",
    "tweet_df_comp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# List of stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words= stopwords.words('english') #import stopwords from NLTK package\n",
    "readInStopwords = pd.read_csv(r\"C:\\Users\\USER\\Desktop\\twitter-sentiment-stock-prediction\\pre_process\\twitterStopWords.csv\", encoding='ISO-8859-1') # import stopwords from CSV file as pandas data frame\n",
    "readInStopwords = readInStopwords.wordList.tolist() # convert pandas data frame to a list\n",
    "readInStopwords.append('http')\n",
    "readInStopwords.append('https')\n",
    "\n",
    "# add in search terms as topic extraction is performed within each search topic, \n",
    "# we do not want the word or valriation of the word captured as a topic word\n",
    "search_terms_revised = ['mortgages','wealthmanagement','pensions','money','transfer']\n",
    "readInStopwords.extend(search_terms)\n",
    "readInStopwords.extend(search_terms_revised)\n",
    "\n",
    "stop_list = stop_words + readInStopwords # combine two lists i.e. NLTK stop words and CSV stopwords\n",
    "stop_list = list(set(stop_list)) # strore only unique values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter for lda, i am selecrign 3 topic and 4 words for each of the search terms \n",
    "number_topics = 5\n",
    "number_words = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", 'able', 'account', 'across', 'advice', 'ai', 'almost', 'also', 'among', 'ca', 'card', 'credit', 'current', 'dear', 'either', 'else', 'ever', 'every', 'get', 'got', 'however', 'least', 'let', 'like', 'likely', 'loan', 'management', 'may', 'might', 'must', \"n't\", 'need', 'neither', 'often', 'personal', 'rather', 'said', 'savings', 'say', 'says', 'sha', 'since', 'tax', 'tis', 'twas', 'us', 'wants', 'wealth', 'wo', 'yet'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mortgage\n",
      "Topics found via LDA:\n",
      "     index  Word 0      Word 1    Word 2      Word 3    Word 4  topic_index\n",
      "0  Topic 0   pogba  historians      join        look  nonsense            0\n",
      "1  Topic 1     man        next  insisted    language   british            1\n",
      "2  Topic 2     buy       china   italian  relatively   renting            2\n",
      "3  Topic 3   think     feeling    normal        grow     whats            3\n",
      "4  Topic 4  income        high      flat          mo   monthly            4\n",
      "current account\n",
      "Topics found via LDA:\n",
      "     index   Word 0   Word 1        Word 2    Word 3    Word 4  topic_index\n",
      "0  Topic 0    agree  whether  transactions   totally  possible            0\n",
      "1  Topic 1  savings    rates   meaningless  relevant       low            1\n",
      "2  Topic 2    agree  whether  transactions   totally  possible            2\n",
      "3  Topic 3    agree  whether  transactions   totally  possible            3\n",
      "4  Topic 4      new      hey          year      next   friends            4\n",
      "savings account\n",
      "Topics found via LDA:\n",
      "     index Word 0   Word 1  Word 2 Word 3    Word 4  topic_index\n",
      "0  Topic 0   mean  balance      dm   earn    please            0\n",
      "1  Topic 1     ah    still  salary  right  relevant            1\n",
      "2  Topic 2   mean  balance      dm   earn    please            2\n",
      "3  Topic 3   mean  balance      dm   earn    please            3\n",
      "4  Topic 4   mean  balance      dm   earn    please            4\n",
      "insurance\n",
      "Topics found via LDA:\n",
      "     index     Word 0      Word 1  Word 2    Word 3     Word 4  topic_index\n",
      "0  Topic 0  polyandry  sequential   claim       end     office            0\n",
      "1  Topic 1     agassi        term    care  catching  wonderful            1\n",
      "2  Topic 2  financial      losing      yu        tl        may            2\n",
      "3  Topic 3     jinmao      change  0817hk      firm       ping            3\n",
      "4  Topic 4      tesla     healthy      de   british      calls            4\n",
      "credit card\n",
      "Topics found via LDA:\n",
      "     index Word 0             Word 1     Word 2            Word 3    Word 4  \\\n",
      "0  Topic 0   yung             brunch       come       comfortable  enjoying   \n",
      "1  Topic 1      n               mijn      abang            mobile    gender   \n",
      "2  Topic 2   face               debt      stuff  surveillancewise  assuming   \n",
      "3  Topic 3   find             google       rent               way     water   \n",
      "4  Topic 4  macau  reluxdiscountcode  wondrlust          checkout      code   \n",
      "\n",
      "   topic_index  \n",
      "0            0  \n",
      "1            1  \n",
      "2            2  \n",
      "3            3  \n",
      "4            4  \n",
      "pension\n",
      "Topics found via LDA:\n",
      "     index       Word 0    Word 1     Word 2       Word 3       Word 4  \\\n",
      "0  Topic 0       rights   defined     period          gay  immigration   \n",
      "1  Topic 1       tories    always  emergency         dumb       bottom   \n",
      "2  Topic 2  responsible      sure   spending  arrangement       raised   \n",
      "3  Topic 3        naive  fiscally   becoming    decisions         mess   \n",
      "4  Topic 4          day        de       para           si        every   \n",
      "\n",
      "   topic_index  \n",
      "0            0  \n",
      "1            1  \n",
      "2            2  \n",
      "3            3  \n",
      "4            4  \n",
      "money transfer\n",
      "Topics found via LDA:\n",
      "     index     Word 0   Word 1    Word 2  Word 3     Word 4  topic_index\n",
      "0  Topic 0    decades  happens     india   since       many            0\n",
      "1  Topic 1    decades  happens     india   since       many            1\n",
      "2  Topic 2    decades  happens     india   since       many            2\n",
      "3  Topic 3    decades  happens     india   since       many            3\n",
      "4  Topic 4  australia       tt  thailand  taiwan  singapore            4\n",
      "tax advice\n",
      "Topics found via LDA:\n",
      "     index  Word 0  Word 1 Word 2 Word 3 Word 4  topic_index\n",
      "0  Topic 0  accept  advice   bill    btw   dont            0\n",
      "1  Topic 1  accept  advice   bill    btw   dont            1\n",
      "2  Topic 2  accept  advice   bill    btw   dont            2\n",
      "3  Topic 3  accept  advice   bill    btw   dont            3\n",
      "4  Topic 4  accept  advice   bill    btw   dont            4\n",
      "investment\n",
      "Topics found via LDA:\n",
      "     index    Word 0     Word 1       Word 2     Word 3   Word 4  topic_index\n",
      "0  Topic 0    prefab     united       cornas    barossa   stride            0\n",
      "1  Topic 1      mgmt    bitdata         bank  factories  keeping            1\n",
      "2  Topic 2     homes       cant  crossborder     binary  reality            2\n",
      "3  Topic 3  piemonte   producer     sourcing        bad   lawyer            3\n",
      "4  Topic 4    police  shortterm          esg       love     land            4\n",
      "wealth management\n",
      "Topics found via LDA:\n",
      "     index      Word 0    Word 1     Word 2      Word 3   Word 4  topic_index\n",
      "0  Topic 0  theasset20     asset     awards       thank  private            0\n",
      "1  Topic 1     ancient  victoria  thousands  spectators  skyline            1\n",
      "2  Topic 2        bank     draft   employee     growing   policy            2\n",
      "3  Topic 3  theasset20     asset     awards       thank  private            3\n",
      "4  Topic 4  theasset20     asset     awards       thank  private            4\n"
     ]
    }
   ],
   "source": [
    "tweets_all_topics= pd.DataFrame()\n",
    "# term frequency modelling\n",
    "for terms in tweet_df_comp['search_term'].unique():\n",
    "    print(terms)\n",
    "    tweets_search_topics  = tweet_df_comp[tweet_df_comp['search_term']==terms].reset_index(drop=True)\n",
    "    corpus = tweets_search_topics['text_clean'].tolist()\n",
    "    # print(corpus)\n",
    "    tf_vectorizer = CountVectorizer(max_df=1, min_df=1, stop_words=stop_list, tokenizer=tokenize_only) \n",
    "    # Use tf (raw term count) features for LDA. !!!!!!!!!!!!!!\n",
    "    tf = tf_vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    # Create and fit the LDA model\n",
    "    model = LDA(n_components=number_topics, n_jobs=-1)\n",
    "    id_topic = model.fit(tf)\n",
    "    # Print the topics found by the LDA model\n",
    "    print(\"Topics found via LDA:\")\n",
    "    topic_keywords = show_topics(vectorizer=tf_vectorizer, lda_model=model, n_words=number_words)        \n",
    "    # Topic - Keywords Dataframe\n",
    "    df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "    df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "    df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "    df_topic_keywords = df_topic_keywords.reset_index()\n",
    "    df_topic_keywords['topic_index'] = df_topic_keywords['index'].str.split(' ', n = 1, expand = True)[[1]].astype('int')\n",
    "    print(df_topic_keywords)\n",
    "    \n",
    "    ############ get the dominat topic for each document in a data frame ###############\n",
    "    # Create Document — Topic Matrix\n",
    "    lda_output = model.transform(tf)\n",
    "    # column names\n",
    "    topicnames = [\"Topic\" + str(i) for i in range(model.n_components)]\n",
    "    # index names\n",
    "    docnames = [\"Doc\" + str(i) for i in range(len(corpus))]\n",
    "    \n",
    "    # Make the pandas dataframe\n",
    "    df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "    # Get dominant topic for each document\n",
    "    dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "    df_document_topic['dominant_topic'] = dominant_topic   \n",
    "    df_document_topic = df_document_topic.reset_index()\n",
    "        \n",
    "    #combine all the search terms into one data frame\n",
    "    tweets_topics = tweets_search_topics.merge(df_document_topic, left_index=True, right_index=True, how='left')\n",
    "    tweets_topics_words = tweets_topics.merge(df_topic_keywords, how='left', left_on='dominant_topic', right_on='topic_index')\n",
    "    tweets_all_topics = tweets_all_topics.append(tweets_topics_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(385, 28)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweets</th>\n",
       "      <th>permalink</th>\n",
       "      <th>date</th>\n",
       "      <th>formatted_date</th>\n",
       "      <th>favorites</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>...</th>\n",
       "      <th>Topic3</th>\n",
       "      <th>Topic4</th>\n",
       "      <th>dominant_topic</th>\n",
       "      <th>index_y</th>\n",
       "      <th>Word 0</th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "      <th>topic_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1177593827768422400</td>\n",
       "      <td>8.579987e+08</td>\n",
       "      <td>it is a free market so loans will exist. howev...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://twitter.com/liujackc/status/1177593827...</td>\n",
       "      <td>2019-09-27 14:40:21+00:00</td>\n",
       "      <td>Fri Sep 27 14:40:21 +0000 2019</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.95</td>\n",
       "      <td>4</td>\n",
       "      <td>Topic 4</td>\n",
       "      <td>income</td>\n",
       "      <td>high</td>\n",
       "      <td>flat</td>\n",
       "      <td>mo</td>\n",
       "      <td>monthly</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1169176926977581057</td>\n",
       "      <td>2.211808e+07</td>\n",
       "      <td>lucky bastard! actually i know someone who did...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://twitter.com/Steve_Dunthorne/status/116...</td>\n",
       "      <td>2019-09-04 09:14:36+00:00</td>\n",
       "      <td>Wed Sep 04 09:14:36 +0000 2019</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3</td>\n",
       "      <td>Topic 3</td>\n",
       "      <td>think</td>\n",
       "      <td>feeling</td>\n",
       "      <td>normal</td>\n",
       "      <td>grow</td>\n",
       "      <td>whats</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1167796354032078850</td>\n",
       "      <td>2.566628e+09</td>\n",
       "      <td>football historians will look back in astonish...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://twitter.com/LysaghtSteph/status/116779...</td>\n",
       "      <td>2019-08-31 13:48:42+00:00</td>\n",
       "      <td>Sat Aug 31 13:48:42 +0000 2019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>Topic 0</td>\n",
       "      <td>pogba</td>\n",
       "      <td>historians</td>\n",
       "      <td>join</td>\n",
       "      <td>look</td>\n",
       "      <td>nonsense</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1164907704894099462</td>\n",
       "      <td>5.383800e+07</td>\n",
       "      <td>#pirata #bravo #italian best italian food with...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://twitter.com/raymoorecn/status/11649077...</td>\n",
       "      <td>2019-08-23 14:30:14+00:00</td>\n",
       "      <td>Fri Aug 23 14:30:14 +0000 2019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#pirata #bravo #italian</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2</td>\n",
       "      <td>Topic 2</td>\n",
       "      <td>buy</td>\n",
       "      <td>china</td>\n",
       "      <td>italian</td>\n",
       "      <td>relatively</td>\n",
       "      <td>renting</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1163704451305054208</td>\n",
       "      <td>2.162408e+09</td>\n",
       "      <td>most hk chinese were in that camp in 1997. hat...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://twitter.com/DrJim_Walker/status/116370...</td>\n",
       "      <td>2019-08-20 06:48:56+00:00</td>\n",
       "      <td>Tue Aug 20 06:48:56 +0000 2019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>Topic 1</td>\n",
       "      <td>man</td>\n",
       "      <td>next</td>\n",
       "      <td>insisted</td>\n",
       "      <td>language</td>\n",
       "      <td>british</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id     author_id  \\\n",
       "0  1177593827768422400  8.579987e+08   \n",
       "1  1169176926977581057  2.211808e+07   \n",
       "2  1167796354032078850  2.566628e+09   \n",
       "3  1164907704894099462  5.383800e+07   \n",
       "4  1163704451305054208  2.162408e+09   \n",
       "\n",
       "                                                text  retweets  \\\n",
       "0  it is a free market so loans will exist. howev...       0.0   \n",
       "1  lucky bastard! actually i know someone who did...       0.0   \n",
       "2  football historians will look back in astonish...       0.0   \n",
       "3  #pirata #bravo #italian best italian food with...       0.0   \n",
       "4  most hk chinese were in that camp in 1997. hat...       0.0   \n",
       "\n",
       "                                           permalink  \\\n",
       "0  https://twitter.com/liujackc/status/1177593827...   \n",
       "1  https://twitter.com/Steve_Dunthorne/status/116...   \n",
       "2  https://twitter.com/LysaghtSteph/status/116779...   \n",
       "3  https://twitter.com/raymoorecn/status/11649077...   \n",
       "4  https://twitter.com/DrJim_Walker/status/116370...   \n",
       "\n",
       "                        date                  formatted_date  favorites  \\\n",
       "0  2019-09-27 14:40:21+00:00  Fri Sep 27 14:40:21 +0000 2019        3.0   \n",
       "1  2019-09-04 09:14:36+00:00  Wed Sep 04 09:14:36 +0000 2019        2.0   \n",
       "2  2019-08-31 13:48:42+00:00  Sat Aug 31 13:48:42 +0000 2019        0.0   \n",
       "3  2019-08-23 14:30:14+00:00  Fri Aug 23 14:30:14 +0000 2019        0.0   \n",
       "4  2019-08-20 06:48:56+00:00  Tue Aug 20 06:48:56 +0000 2019        0.0   \n",
       "\n",
       "  mentions                 hashtags  ...  Topic3 Topic4 dominant_topic  \\\n",
       "0      NaN                      NaN  ...    0.01   0.95              4   \n",
       "1      NaN                      NaN  ...    0.91   0.02              3   \n",
       "2      NaN                      NaN  ...    0.01   0.01              0   \n",
       "3      NaN  #pirata #bravo #italian  ...    0.02   0.02              2   \n",
       "4      NaN                      NaN  ...    0.01   0.01              1   \n",
       "\n",
       "   index_y  Word 0      Word 1    Word 2      Word 3    Word 4  topic_index  \n",
       "0  Topic 4  income        high      flat          mo   monthly            4  \n",
       "1  Topic 3   think     feeling    normal        grow     whats            3  \n",
       "2  Topic 0   pogba  historians      join        look  nonsense            0  \n",
       "3  Topic 2     buy       china   italian  relatively   renting            2  \n",
       "4  Topic 1     man        next  insisted    language   british            1  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_all_topics = tweets_all_topics.reset_index(drop=True)\n",
    "print(tweets_all_topics.shape)\n",
    "tweets_all_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_all_topics.to_csv(r'C:\\Users\\USER\\Desktop\\twitter-sentiment-stock-prediction\\processed_data\\tweets_all_topics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis using Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import h5py\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:327: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, 56, 300)           120000300 \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 256)               439296    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 120,576,310\n",
      "Trainable params: 576,010\n",
      "Non-trainable params: 120,000,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# read in the weight of the trained model.\n",
    "weight_path = r'C:\\Users\\USER\\Desktop\\twitter-sentiment-stock-prediction\\models\\dl_sentiment_model\\best_weight_glove_bi_512.hdf5'\n",
    "prd_model = load_model(weight_path)\n",
    "prd_model.summary()\n",
    "word_idx = json.load(open(r\"C:\\Users\\USER\\Desktop\\twitter-sentiment-stock-prediction\\models\\dl_sentiment_model\\word_idx.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_DL(prd_model, text_data, word_idx):\n",
    "\n",
    "    #data = \"Pass the salt\"\n",
    "\n",
    "    live_list = []\n",
    "    batchSize = len(text_data)\n",
    "    live_list_np = np.zeros((56,batchSize))\n",
    "    for index, row in text_data.iterrows():\n",
    "        #print (index)\n",
    "        text_data_sample = text_data['text'][index]\n",
    "        # split the sentence into its words and remove any punctuations.\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        text_data_list = tokenizer.tokenize(text_data_sample)\n",
    "\n",
    "        #text_data_list = text_data_sample.split()\n",
    "\n",
    "\n",
    "        labels = np.array(['1','2','3','4','5','6','7','8','9','10'], dtype = \"int\")\n",
    "        #word_idx['I']\n",
    "        # get index for the live stage\n",
    "        data_index = np.array([word_idx[word.lower()] if word.lower() in word_idx else 0 for word in text_data_list])\n",
    "        data_index_np = np.array(data_index)\n",
    "\n",
    "        # padded with zeros of length 56 i.e maximum length\n",
    "        padded_array = np.zeros(56)\n",
    "        padded_array[:data_index_np.shape[0]] = data_index_np[:56]\n",
    "        data_index_np_pad = padded_array.astype(int)\n",
    "\n",
    "\n",
    "        live_list.append(data_index_np_pad)\n",
    "\n",
    "    live_list_np = np.asarray(live_list)\n",
    "    score = prd_model.predict(live_list_np, batch_size=batchSize, verbose=0)\n",
    "    single_score = np.round(np.dot(score, labels)/10,decimals=2)\n",
    "\n",
    "    score_all  = []\n",
    "    for each_score in score:\n",
    "\n",
    "        top_3_index = np.argsort(each_score)[-3:]\n",
    "        top_3_scores = each_score[top_3_index]\n",
    "        top_3_weights = top_3_scores/np.sum(top_3_scores)\n",
    "        single_score_dot = np.round(np.dot(top_3_index, top_3_weights)/10, decimals = 2)\n",
    "        score_all.append(single_score_dot)\n",
    "\n",
    "    text_data['Sentiment_Score'] = pd.DataFrame(score_all)\n",
    "\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data =  tweets_all_topics\n",
    "# Deep Learning sentiment scoring\n",
    "text_out = get_sentiment_DL(prd_model, text_data, word_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### example of negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>147</th>\n",
       "      <th>94</th>\n",
       "      <th>72</th>\n",
       "      <th>160</th>\n",
       "      <th>157</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>why is it always the answer to say the tories ...</td>\n",
       "      <td>when relationships ended i gave myself 24 hour...</td>\n",
       "      <td>oh yeah. you’re right. damn the pesky eu good ...</td>\n",
       "      <td>a. not sure why you are comparing labour to to...</td>\n",
       "      <td>that’s completely untrue. i was disgusted at l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentiment_Score</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               147  \\\n",
       "text             why is it always the answer to say the tories ...   \n",
       "Sentiment_Score                                               0.07   \n",
       "\n",
       "                                                               94   \\\n",
       "text             when relationships ended i gave myself 24 hour...   \n",
       "Sentiment_Score                                               0.08   \n",
       "\n",
       "                                                               72   \\\n",
       "text             oh yeah. you’re right. damn the pesky eu good ...   \n",
       "Sentiment_Score                                               0.09   \n",
       "\n",
       "                                                               160  \\\n",
       "text             a. not sure why you are comparing labour to to...   \n",
       "Sentiment_Score                                               0.09   \n",
       "\n",
       "                                                               157  \n",
       "text             that’s completely untrue. i was disgusted at l...  \n",
       "Sentiment_Score                                                0.1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_out.sort_values(by='Sentiment_Score')[['text','Sentiment_Score']].head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  example of positive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>193</th>\n",
       "      <th>366</th>\n",
       "      <th>207</th>\n",
       "      <th>230</th>\n",
       "      <th>254</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>\"one of the finest clos apaltas of all time,\" ...</td>\n",
       "      <td>one of the great pessac léognans of 2014, the ...</td>\n",
       "      <td>one of the greatest salons ever released, the ...</td>\n",
       "      <td>one of the most impressive international colla...</td>\n",
       "      <td>one of the greatest producers in portugal, tay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentiment_Score</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               193  \\\n",
       "text             \"one of the finest clos apaltas of all time,\" ...   \n",
       "Sentiment_Score                                               0.86   \n",
       "\n",
       "                                                               366  \\\n",
       "text             one of the great pessac léognans of 2014, the ...   \n",
       "Sentiment_Score                                               0.85   \n",
       "\n",
       "                                                               207  \\\n",
       "text             one of the greatest salons ever released, the ...   \n",
       "Sentiment_Score                                               0.84   \n",
       "\n",
       "                                                               230  \\\n",
       "text             one of the most impressive international colla...   \n",
       "Sentiment_Score                                               0.83   \n",
       "\n",
       "                                                               254  \n",
       "text             one of the greatest producers in portugal, tay...  \n",
       "Sentiment_Score                                               0.83  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_out.sort_values(by='Sentiment_Score', ascending=False)[['text','Sentiment_Score']].head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_out.to_csv(r'C:\\Users\\USER\\Desktop\\twitter-sentiment-stock-prediction\\processed_data\\tweets_topics_sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NER(text_data):\n",
    "    #/Users/prajwalshreyas/Desktop/Singularity/dockerApps/ner-algo/stanford-ner-2015-01-30\n",
    "    stanford_classifier = r'C:\\Users\\USER\\Desktop\\twitter-sentiment-stock-prediction\\models\\ner\\english.all.3class.distsim.crf.ser.gz'\n",
    "    stanford_ner_path = r'C:\\Users\\USER\\Desktop\\twitter-sentiment-stock-prediction\\models\\ner\\stanford-ner.jar'\n",
    "\n",
    "    \n",
    "    st = StanfordNERTagger(stanford_classifier, stanford_ner_path, encoding='utf-16')\n",
    "    \n",
    "    print ('start get_NER')\n",
    "    text_out = text_data.copy()\n",
    "    doc = [ docs + ' 12345678 ' for docs in list(text_data['text'])]\n",
    "    # ------------------------- Stanford Named Entity Recognition\n",
    "    tokens = nltk.word_tokenize(str(doc))\n",
    "    entities = st.tag(tokens) # actual tagging takes place using Stanford NER algorithm\n",
    "\n",
    "\n",
    "    entities = [list(elem) for elem in entities] # Convert list of tuples to list of list\n",
    "    print ('tag complete')\n",
    "    for idx,element in enumerate(entities):\n",
    "        try:\n",
    "            if entities[idx][0] == '12345678':\n",
    "                entities[idx][1] = \"DOC_NUMBER\"  #  Modify data by adding the tag \"Doc_Number\"\n",
    "            #elif entities[idx][0].lower() == keyword:\n",
    "            #    entities[idx][1] = \"KEYWORD\"\n",
    "            # Combine First and Last name into a single word\n",
    "            elif entities[idx][1] == \"PERSON\" and entities[idx + 1][1] == \"PERSON\":\n",
    "                entities[idx + 1][0] = entities[idx][0] + '-' + entities[idx+1][0]\n",
    "                entities[idx][1] = 'Combined'\n",
    "            # Combine consecutive Organization names\n",
    "            elif entities[idx][1] == 'ORGANIZATION' and entities[idx + 1][1] == 'ORGANIZATION':\n",
    "                entities[idx + 1][0] = entities[idx][0] + '-' + entities[idx+1][0]\n",
    "                entities[idx][1] = 'Combined'\n",
    "        except IndexError:\n",
    "            break\n",
    "    print ('enumerate complete')\n",
    "    # Filter list of list for the words we are interested in\n",
    "    filter_list = ['DOC_NUMBER','PERSON','LOCATION','ORGANIZATION']\n",
    "    entityWordList = [element for element in entities if any(i in element for i in filter_list)]\n",
    "\n",
    "    entityString = ' '.join(str(word) for insideList in entityWordList for word in insideList) # convert list to string and concatenate it\n",
    "    entitySubString = entityString.split(\"DOC_NUMBER\") # split the string using the separator 'TWEET_NUMBER'\n",
    "    del entitySubString[-1] # delete the extra blank row created in the previous step\n",
    "\n",
    "    # Store the classified NERs in the main tweet data frame\n",
    "    for idx,docNER in enumerate(entitySubString):\n",
    "        docNER = docNER.strip().split() # split the string into word list\n",
    "        # Filter for words tagged as Organization and store it in data frame\n",
    "        text_out.loc[idx,'Organization'] =  ','.join([docNER[i-1]  for i,x in enumerate(docNER) if x == 'ORGANIZATION'])\n",
    "        # Filter for words tagged as LOCATION and store it in data frame\n",
    "        text_out.loc[idx,'Place'] = ','.join([docNER[i-1] for i,x in enumerate(docNER) if x == 'LOCATION'])\n",
    "        # Filter for words tagged as PERSON and store it in data frame\n",
    "        text_out.loc[idx,'Person'] = ','.join([docNER[i-1]  for i,x in enumerate(docNER) if x == 'PERSON'])\n",
    "\n",
    "    print ('process complete')\n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start get_NER\n",
      "tag complete\n",
      "enumerate complete\n",
      "process complete\n"
     ]
    }
   ],
   "source": [
    "text_ner_out = get_NER(text_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    id     author_id  \\\n",
      "0  1177593827768422400  8.579987e+08   \n",
      "1  1169176926977581057  2.211808e+07   \n",
      "2  1167796354032078850  2.566628e+09   \n",
      "3  1164907704894099462  5.383800e+07   \n",
      "4  1163704451305054208  2.162408e+09   \n",
      "\n",
      "                                                text  retweets  \\\n",
      "0  it is a free market so loans will exist. howev...       0.0   \n",
      "1  lucky bastard! actually i know someone who did...       0.0   \n",
      "2  football historians will look back in astonish...       0.0   \n",
      "3  #pirata #bravo #italian best italian food with...       0.0   \n",
      "4  most hk chinese were in that camp in 1997. hat...       0.0   \n",
      "\n",
      "                                           permalink  \\\n",
      "0  https://twitter.com/liujackc/status/1177593827...   \n",
      "1  https://twitter.com/Steve_Dunthorne/status/116...   \n",
      "2  https://twitter.com/LysaghtSteph/status/116779...   \n",
      "3  https://twitter.com/raymoorecn/status/11649077...   \n",
      "4  https://twitter.com/DrJim_Walker/status/116370...   \n",
      "\n",
      "                        date                  formatted_date  favorites  \\\n",
      "0  2019-09-27 14:40:21+00:00  Fri Sep 27 14:40:21 +0000 2019        3.0   \n",
      "1  2019-09-04 09:14:36+00:00  Wed Sep 04 09:14:36 +0000 2019        2.0   \n",
      "2  2019-08-31 13:48:42+00:00  Sat Aug 31 13:48:42 +0000 2019        0.0   \n",
      "3  2019-08-23 14:30:14+00:00  Fri Aug 23 14:30:14 +0000 2019        0.0   \n",
      "4  2019-08-20 06:48:56+00:00  Tue Aug 20 06:48:56 +0000 2019        0.0   \n",
      "\n",
      "  mentions                 hashtags  ...  Topic4 dominant_topic  index_y  \\\n",
      "0      NaN                      NaN  ...    0.95              4  Topic 4   \n",
      "1      NaN                      NaN  ...    0.02              3  Topic 3   \n",
      "2      NaN                      NaN  ...    0.01              0  Topic 0   \n",
      "3      NaN  #pirata #bravo #italian  ...    0.02              2  Topic 2   \n",
      "4      NaN                      NaN  ...    0.01              1  Topic 1   \n",
      "\n",
      "   Word 0      Word 1    Word 2      Word 3    Word 4  topic_index  \\\n",
      "0  income        high      flat          mo   monthly            4   \n",
      "1   think     feeling    normal        grow     whats            3   \n",
      "2   pogba  historians      join        look  nonsense            0   \n",
      "3     buy       china   italian  relatively   renting            2   \n",
      "4     man        next  insisted    language   british            1   \n",
      "\n",
      "   Sentiment_Score  \n",
      "0             0.21  \n",
      "1             0.30  \n",
      "2             0.20  \n",
      "3             0.61  \n",
      "4             0.30  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "print(text_ner_out.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### output  the file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ner_out.to_csv(r'C:\\Users\\USER\\Desktop\\twitter-sentiment-stock-prediction\\processed_data\\tweets_topics_sentiment_ner.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
