{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape tweets for a period of 6 months. This has resulted in approximately 43.7K tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GetOldTweets3 as got\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# NTLK function\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tokenize as tok\n",
    "from nltk.stem.snowball import SnowballStemmer # load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "import lda # topic modeling -NMF & LDA\n",
    "import string\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "# Tf-Idf and Clustering packages\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text clustering with K-means and tf-idf\n",
    "https://medium.com/@MSalnikov/text-clustering-with-k-means-and-tf-idf-f099bcf95183"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = ['mortgage','current account','savings account','insurance','credit card','pension',\n",
    "                'personal loan','money transfer','tax advice','investment','wealth management']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mortgage\n",
      "current account\n",
      "savings account\n",
      "insurance\n",
      "credit card\n",
      "pension\n",
      "personal loan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "money transfer\n",
      "tax advice\n",
      "investment\n",
      "wealth management\n"
     ]
    }
   ],
   "source": [
    "# scrape the data (terms)\n",
    "tweet_df_all = pd.DataFrame()\n",
    "for term in search_terms:\n",
    "    print(term)\n",
    "    tweetCriteria = got.manager.TweetCriteria().setQuerySearch(term)\\\n",
    "                                               .setSince(\"2019-04-13\")\\\n",
    "                                               .setUntil(\"2019-09-28\")\\\n",
    "                                               .setNear(\"Hong Kong\")\\\n",
    "                                               .setWithin(\"310mi\")\n",
    "    tweet = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "    tweet_list = [[tweet[x].id,\n",
    "                  tweet[x].author_id,\n",
    "                  tweet[x].text,\n",
    "                  tweet[x].retweets,\n",
    "                  tweet[x].permalink,\n",
    "                  tweet[x].date,\n",
    "                  tweet[x].formatted_date,\n",
    "                  tweet[x].favorites,\n",
    "                  tweet[x].mentions,\n",
    "                  tweet[x].hashtags,\n",
    "                  tweet[x].geo,\n",
    "                  tweet[x].urls\n",
    "                 ]for x in range(0, len(tweet))]\n",
    "    tweet_df = pd.DataFrame(tweet_list)\n",
    "    tweet_df['search_term'] = term\n",
    "    tweet_df_all = tweet_df_all.append(tweet_df)\n",
    "\n",
    "\n",
    "tweet_df_all.columns = ['id','author_id','text','retweets','permalink','date','formatted_date','favorites','mentions','hashtags','geo','urls','search_term']\n",
    "\n",
    "                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384, 13)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df_all.to_csv(r\"C:\\Users\\USER\\Desktop\\twitter-sentiment-stock-prediction\\data\\all_tweet.csv\", index=False)\n",
    "tweet_df_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add List of Financial Institutions providng the above products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df_all = pd.read_csv(r'C:\\Users\\USER\\Desktop\\twitter-sentiment-stock-prediction\\data\\all_tweet.csv')\n",
    "tweet_df_all = tweet_df_all[tweet_df_all['text'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweets</th>\n",
       "      <th>permalink</th>\n",
       "      <th>date</th>\n",
       "      <th>formatted_date</th>\n",
       "      <th>favorites</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>geo</th>\n",
       "      <th>urls</th>\n",
       "      <th>search_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1177593827768422400</td>\n",
       "      <td>8.579987e+08</td>\n",
       "      <td>It is a free market so loans will exist. Howev...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://twitter.com/liujackc/status/1177593827...</td>\n",
       "      <td>2019-09-27 14:40:21+00:00</td>\n",
       "      <td>Fri Sep 27 14:40:21 +0000 2019</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1169176926977581057</td>\n",
       "      <td>2.211808e+07</td>\n",
       "      <td>Lucky bastard! Actually I know someone who did...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://twitter.com/Steve_Dunthorne/status/116...</td>\n",
       "      <td>2019-09-04 09:14:36+00:00</td>\n",
       "      <td>Wed Sep 04 09:14:36 +0000 2019</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1167796354032078850</td>\n",
       "      <td>2.566628e+09</td>\n",
       "      <td>Football historians will look back in astonish...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://twitter.com/LysaghtSteph/status/116779...</td>\n",
       "      <td>2019-08-31 13:48:42+00:00</td>\n",
       "      <td>Sat Aug 31 13:48:42 +0000 2019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1164907704894099462</td>\n",
       "      <td>5.383800e+07</td>\n",
       "      <td>#pirata #bravo #italian Best Italian food with...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://twitter.com/raymoorecn/status/11649077...</td>\n",
       "      <td>2019-08-23 14:30:14+00:00</td>\n",
       "      <td>Fri Aug 23 14:30:14 +0000 2019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#pirata #bravo #italian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.instagram.com/p/B1gojG0gLfRj02mhIX...</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1163704451305054208</td>\n",
       "      <td>2.162408e+09</td>\n",
       "      <td>Most HK Chinese were in that camp in 1997. Hat...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://twitter.com/DrJim_Walker/status/116370...</td>\n",
       "      <td>2019-08-20 06:48:56+00:00</td>\n",
       "      <td>Tue Aug 20 06:48:56 +0000 2019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id     author_id  \\\n",
       "0  1177593827768422400  8.579987e+08   \n",
       "1  1169176926977581057  2.211808e+07   \n",
       "2  1167796354032078850  2.566628e+09   \n",
       "3  1164907704894099462  5.383800e+07   \n",
       "4  1163704451305054208  2.162408e+09   \n",
       "\n",
       "                                                text  retweets  \\\n",
       "0  It is a free market so loans will exist. Howev...       0.0   \n",
       "1  Lucky bastard! Actually I know someone who did...       0.0   \n",
       "2  Football historians will look back in astonish...       0.0   \n",
       "3  #pirata #bravo #italian Best Italian food with...       0.0   \n",
       "4  Most HK Chinese were in that camp in 1997. Hat...       0.0   \n",
       "\n",
       "                                           permalink  \\\n",
       "0  https://twitter.com/liujackc/status/1177593827...   \n",
       "1  https://twitter.com/Steve_Dunthorne/status/116...   \n",
       "2  https://twitter.com/LysaghtSteph/status/116779...   \n",
       "3  https://twitter.com/raymoorecn/status/11649077...   \n",
       "4  https://twitter.com/DrJim_Walker/status/116370...   \n",
       "\n",
       "                        date                  formatted_date  favorites  \\\n",
       "0  2019-09-27 14:40:21+00:00  Fri Sep 27 14:40:21 +0000 2019        3.0   \n",
       "1  2019-09-04 09:14:36+00:00  Wed Sep 04 09:14:36 +0000 2019        2.0   \n",
       "2  2019-08-31 13:48:42+00:00  Sat Aug 31 13:48:42 +0000 2019        0.0   \n",
       "3  2019-08-23 14:30:14+00:00  Fri Aug 23 14:30:14 +0000 2019        0.0   \n",
       "4  2019-08-20 06:48:56+00:00  Tue Aug 20 06:48:56 +0000 2019        0.0   \n",
       "\n",
       "  mentions                 hashtags  geo  \\\n",
       "0      NaN                      NaN  NaN   \n",
       "1      NaN                      NaN  NaN   \n",
       "2      NaN                      NaN  NaN   \n",
       "3      NaN  #pirata #bravo #italian  NaN   \n",
       "4      NaN                      NaN  NaN   \n",
       "\n",
       "                                                urls search_term  \n",
       "0                                                NaN    mortgage  \n",
       "1                                                NaN    mortgage  \n",
       "2                                                NaN    mortgage  \n",
       "3  https://www.instagram.com/p/B1gojG0gLfRj02mhIX...    mortgage  \n",
       "4                                                NaN    mortgage  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tweet_df_all.shape);tweet_df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "search_term\n",
       "credit card           42\n",
       "current account        3\n",
       "insurance             71\n",
       "investment           216\n",
       "money transfer         2\n",
       "mortgage              18\n",
       "pension               24\n",
       "savings account        2\n",
       "tax advice             1\n",
       "wealth management      5\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df_all['text'] = tweet_df_all['text'].str.lower()\n",
    "tweet_df_comp = tweet_df_all\n",
    "tweet_df_comp.groupby('search_term')['id'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Extraction with LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove unnessary words\n",
    "#Complie all regular expressions\n",
    "isURL = re.compile(r'http[s]?:// (?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', re.VERBOSE | re.IGNORECASE)\n",
    "isRTusername = re.compile(r'^RT+[\\s]+(@[\\w_]+:)',re.VERBOSE | re.IGNORECASE) #r'^RT+[\\s]+(@[\\w_]+:)'\n",
    "isEntity = re.compile(r'@[\\w_]+', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "# Helper functions\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]])) \n",
    "\n",
    "# Show top n keywords for each topic\n",
    "def show_topics(vectorizer, lda_model, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "     \n",
    "        \n",
    "def clean_tweet(row):\n",
    "    row = isURL.sub(\"\",row)\n",
    "    row = isRTusername.sub(\"\",row)\n",
    "    row = isEntity.sub(\"\",row)\n",
    "    return row\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in tok.sent_tokenize(text) for word in tok.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweets</th>\n",
       "      <th>permalink</th>\n",
       "      <th>date</th>\n",
       "      <th>formatted_date</th>\n",
       "      <th>favorites</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>geo</th>\n",
       "      <th>urls</th>\n",
       "      <th>search_term</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1177593827768422400</td>\n",
       "      <td>8.579987e+08</td>\n",
       "      <td>it is a free market so loans will exist. howev...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://twitter.com/liujackc/status/1177593827...</td>\n",
       "      <td>2019-09-27 14:40:21+00:00</td>\n",
       "      <td>Fri Sep 27 14:40:21 +0000 2019</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mortgage</td>\n",
       "      <td>it is a free market so loans will exist howeve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1169176926977581057</td>\n",
       "      <td>2.211808e+07</td>\n",
       "      <td>lucky bastard! actually i know someone who did...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://twitter.com/Steve_Dunthorne/status/116...</td>\n",
       "      <td>2019-09-04 09:14:36+00:00</td>\n",
       "      <td>Wed Sep 04 09:14:36 +0000 2019</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mortgage</td>\n",
       "      <td>lucky bastard actually i know someone who did ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1167796354032078850</td>\n",
       "      <td>2.566628e+09</td>\n",
       "      <td>football historians will look back in astonish...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://twitter.com/LysaghtSteph/status/116779...</td>\n",
       "      <td>2019-08-31 13:48:42+00:00</td>\n",
       "      <td>Sat Aug 31 13:48:42 +0000 2019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mortgage</td>\n",
       "      <td>football historians will look back in astonish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1164907704894099462</td>\n",
       "      <td>5.383800e+07</td>\n",
       "      <td>#pirata #bravo #italian best italian food with...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://twitter.com/raymoorecn/status/11649077...</td>\n",
       "      <td>2019-08-23 14:30:14+00:00</td>\n",
       "      <td>Fri Aug 23 14:30:14 +0000 2019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#pirata #bravo #italian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.instagram.com/p/B1gojG0gLfRj02mhIX...</td>\n",
       "      <td>mortgage</td>\n",
       "      <td>pirata bravo italian best italian food without...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1163704451305054208</td>\n",
       "      <td>2.162408e+09</td>\n",
       "      <td>most hk chinese were in that camp in 1997. hat...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://twitter.com/DrJim_Walker/status/116370...</td>\n",
       "      <td>2019-08-20 06:48:56+00:00</td>\n",
       "      <td>Tue Aug 20 06:48:56 +0000 2019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mortgage</td>\n",
       "      <td>most hk chinese were in that camp in 1997 hate...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id     author_id  \\\n",
       "0  1177593827768422400  8.579987e+08   \n",
       "1  1169176926977581057  2.211808e+07   \n",
       "2  1167796354032078850  2.566628e+09   \n",
       "3  1164907704894099462  5.383800e+07   \n",
       "4  1163704451305054208  2.162408e+09   \n",
       "\n",
       "                                                text  retweets  \\\n",
       "0  it is a free market so loans will exist. howev...       0.0   \n",
       "1  lucky bastard! actually i know someone who did...       0.0   \n",
       "2  football historians will look back in astonish...       0.0   \n",
       "3  #pirata #bravo #italian best italian food with...       0.0   \n",
       "4  most hk chinese were in that camp in 1997. hat...       0.0   \n",
       "\n",
       "                                           permalink  \\\n",
       "0  https://twitter.com/liujackc/status/1177593827...   \n",
       "1  https://twitter.com/Steve_Dunthorne/status/116...   \n",
       "2  https://twitter.com/LysaghtSteph/status/116779...   \n",
       "3  https://twitter.com/raymoorecn/status/11649077...   \n",
       "4  https://twitter.com/DrJim_Walker/status/116370...   \n",
       "\n",
       "                        date                  formatted_date  favorites  \\\n",
       "0  2019-09-27 14:40:21+00:00  Fri Sep 27 14:40:21 +0000 2019        3.0   \n",
       "1  2019-09-04 09:14:36+00:00  Wed Sep 04 09:14:36 +0000 2019        2.0   \n",
       "2  2019-08-31 13:48:42+00:00  Sat Aug 31 13:48:42 +0000 2019        0.0   \n",
       "3  2019-08-23 14:30:14+00:00  Fri Aug 23 14:30:14 +0000 2019        0.0   \n",
       "4  2019-08-20 06:48:56+00:00  Tue Aug 20 06:48:56 +0000 2019        0.0   \n",
       "\n",
       "  mentions                 hashtags  geo  \\\n",
       "0      NaN                      NaN  NaN   \n",
       "1      NaN                      NaN  NaN   \n",
       "2      NaN                      NaN  NaN   \n",
       "3      NaN  #pirata #bravo #italian  NaN   \n",
       "4      NaN                      NaN  NaN   \n",
       "\n",
       "                                                urls search_term  \\\n",
       "0                                                NaN    mortgage   \n",
       "1                                                NaN    mortgage   \n",
       "2                                                NaN    mortgage   \n",
       "3  https://www.instagram.com/p/B1gojG0gLfRj02mhIX...    mortgage   \n",
       "4                                                NaN    mortgage   \n",
       "\n",
       "                                          text_clean  \n",
       "0  it is a free market so loans will exist howeve...  \n",
       "1  lucky bastard actually i know someone who did ...  \n",
       "2  football historians will look back in astonish...  \n",
       "3  pirata bravo italian best italian food without...  \n",
       "4  most hk chinese were in that camp in 1997 hate...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df_comp['text_clean'] = tweet_df_comp['text'].apply(lambda row:clean_tweet(row))\n",
    "\n",
    "#remove punctuations\n",
    "RE_PUNCTUATION = '|'.join([re.escape(x) for x in string.punctuation])  \n",
    "tweet_df_comp['text_clean'] = tweet_df_comp['text_clean'].str.replace(RE_PUNCTUATION, \"\")\n",
    "tweet_df_comp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# List of stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words= stopwords.words('english') #import stopwords from NLTK package\n",
    "readInStopwords = pd.read_csv(r\"C:\\Users\\USER\\Desktop\\twitter-sentiment-stock-prediction\\pre_process\\twitterStopWords.csv\", encoding='ISO-8859-1') # import stopwords from CSV file as pandas data frame\n",
    "readInStopwords = readInStopwords.wordList.tolist() # convert pandas data frame to a list\n",
    "readInStopwords.append('http')\n",
    "readInStopwords.append('https')\n",
    "\n",
    "# add in search terms as topic extraction is performed within each search topic, \n",
    "# we do not want the word or valriation of the word captured as a topic word\n",
    "search_terms_revised = ['mortgages','wealthmanagement','pensions','money','transfer']\n",
    "readInStopwords.extend(search_terms)\n",
    "readInStopwords.extend(search_terms_revised)\n",
    "\n",
    "stop_list = stop_words + readInStopwords # combine two lists i.e. NLTK stop words and CSV stopwords\n",
    "stop_list = list(set(stop_list)) # strore only unique values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter for lda, i am selecrign 3 topic and 4 words for each of the search terms \n",
    "number_topics = 5\n",
    "number_words = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mortgage\n",
      "Topics found via LDA:\n",
      "     index   Word 0     Word 1   Word 2     Word 3     Word 4  topic_index\n",
      "0  Topic 0   income        buy  parents      large   payments            0\n",
      "1  Topic 1    pogba      £200k  chapter    sanchez   business            1\n",
      "2  Topic 2    china  accounted    stock  currently      scale            2\n",
      "3  Topic 3  italian     prices    flats    chinese  vengeance            3\n",
      "4  Topic 4    think    feeling      man      space       good            4\n",
      "current account\n",
      "Topics found via LDA:\n",
      "     index   Word 0   Word 1        Word 2    Word 3    Word 4  topic_index\n",
      "0  Topic 0    agree  whether  transactions   totally  possible            0\n",
      "1  Topic 1    agree  whether  transactions   totally  possible            1\n",
      "2  Topic 2  savings    rates   meaningless  relevant       low            2\n",
      "3  Topic 3      new     year          next       hey   friends            3\n",
      "4  Topic 4    agree  whether  transactions   totally  possible            4\n",
      "savings account\n",
      "Topics found via LDA:\n",
      "     index Word 0   Word 1  Word 2 Word 3    Word 4  topic_index\n",
      "0  Topic 0   mean  balance      dm   earn    please            0\n",
      "1  Topic 1   mean  balance      dm   earn    please            1\n",
      "2  Topic 2   mean  balance      dm   earn    please            2\n",
      "3  Topic 3     ah    still  salary  right  relevant            3\n",
      "4  Topic 4   mean  balance      dm   earn    please            4\n",
      "insurance\n",
      "Topics found via LDA:\n",
      "     index  Word 0      Word 1     Word 2   Word 3   Word 4  topic_index\n",
      "0  Topic 0  jinmao  sequential  polyandry      end   0817hk            0\n",
      "1  Topic 1   tesla     healthy     agassi    least    arent            1\n",
      "2  Topic 2   calls     british     rights  britain  holders            2\n",
      "3  Topic 3      de        term     change   losing   margen            3\n",
      "4  Topic 4   claim   financial       care   office      idp            4\n",
      "credit card\n",
      "Topics found via LDA:\n",
      "     index  Word 0  Word 1  Word 2  Word 3            Word 4  topic_index\n",
      "0  Topic 0    face  mobile   stuff      id  surveillancewise            0\n",
      "1  Topic 1    yung  online      ni      ng             naman            1\n",
      "2  Topic 2    debt       n    mijn   macau  reluxvouchercode            2\n",
      "3  Topic 3   abang    find  2xyear  brunch              come            3\n",
      "4  Topic 4  google   drive   turns  iphone           invoice            4\n",
      "pension\n",
      "Topics found via LDA:\n",
      "     index  Word 0       Word 1   Word 2  Word 3     Word 4  topic_index\n",
      "0  Topic 0     10x       system  several  quotes  purchases            0\n",
      "1  Topic 1  tories  responsible     race     rob        say            1\n",
      "2  Topic 2    lang        sapat       na   dagat    serbesa            2\n",
      "3  Topic 3     day        every   rights   naive   fiscally            3\n",
      "4  Topic 4      si         para       de    sure    defined            4\n",
      "money transfer\n",
      "Topics found via LDA:\n",
      "     index     Word 0   Word 1    Word 2  Word 3     Word 4  topic_index\n",
      "0  Topic 0  australia       tt  thailand  taiwan  singapore            0\n",
      "1  Topic 1    decades  happens     india   since       many            1\n",
      "2  Topic 2    decades  happens     india   since       many            2\n",
      "3  Topic 3    decades  happens     india   since       many            3\n",
      "4  Topic 4    decades  happens     india   since       many            4\n",
      "tax advice\n",
      "Topics found via LDA:\n",
      "     index  Word 0  Word 1 Word 2 Word 3 Word 4  topic_index\n",
      "0  Topic 0  accept  advice   bill    btw   dont            0\n",
      "1  Topic 1  accept  advice   bill    btw   dont            1\n",
      "2  Topic 2  accept  advice   bill    btw   dont            2\n",
      "3  Topic 3  accept  advice   bill    btw   dont            3\n",
      "4  Topic 4  accept  advice   bill    btw   dont            4\n",
      "investment\n",
      "Topics found via LDA:\n",
      "     index     Word 0    Word 1       Word 2   Word 3     Word 4  topic_index\n",
      "0  Topic 0   piemonte  producer     sourcing  college      sleep            0\n",
      "1  Topic 1     police      mgmt         cant    homes    bitdata            1\n",
      "2  Topic 2     prefab      bank    heidsieck   stride  collector            2\n",
      "3  Topic 3     binary    cornas      reality   shiraz     lawyer            3\n",
      "4  Topic 4  shortterm       esg  crossborder  barossa    keeping            4\n",
      "wealth management\n",
      "Topics found via LDA:\n",
      "     index   Word 0        Word 1        Word 2        Word 3          Word 4  \\\n",
      "0  Topic 0  growing       ancient      victoria     thousands      spectators   \n",
      "1  Topic 1    thank       private       joining    theasset20             etf   \n",
      "2  Topic 2     bank      employee         draft    regulation         sharing   \n",
      "3  Topic 3    thank       private       joining    theasset20             etf   \n",
      "4  Topic 4    wills  generational  preservation  familywealth  estateplanning   \n",
      "\n",
      "   topic_index  \n",
      "0            0  \n",
      "1            1  \n",
      "2            2  \n",
      "3            3  \n",
      "4            4  \n"
     ]
    }
   ],
   "source": [
    "tweets_all_topics= pd.DataFrame()\n",
    "# term frequency modelling\n",
    "for terms in tweet_df_comp['search_term'].unique():\n",
    "    print(terms)\n",
    "    tweets_search_topics  = tweet_df_comp[tweet_df_comp['search_term']==terms].reset_index(drop=True)\n",
    "    corpus = tweets_search_topics['text_clean'].tolist()\n",
    "    # print(corpus)\n",
    "    tf_vectorizer = CountVectorizer(max_df=1, min_df=1, stop_words=stop_list, tokenizer=tokenize_only) \n",
    "    # Use tf (raw term count) features for LDA. !!!!!!!!!!!!!!\n",
    "    tf = tf_vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    # Create and fit the LDA model\n",
    "    model = LDA(n_components=number_topics, n_jobs=-1)\n",
    "    id_topic = model.fit(tf)\n",
    "    # Print the topics found by the LDA model\n",
    "    print(\"Topics found via LDA:\")\n",
    "    topic_keywords = show_topics(vectorizer=tf_vectorizer, lda_model=model, n_words=number_words)        \n",
    "    # Topic - Keywords Dataframe\n",
    "    df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "    df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "    df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "    df_topic_keywords = df_topic_keywords.reset_index()\n",
    "    df_topic_keywords['topic_index'] = df_topic_keywords['index'].str.split(' ', n = 1, expand = True)[[1]].astype('int')\n",
    "    print(df_topic_keywords)\n",
    "    \n",
    "    ############ get the dominat topic for each document in a data frame ###############\n",
    "    # Create Document — Topic Matrix\n",
    "    lda_output = model.transform(tf)\n",
    "    # column names\n",
    "    topicnames = [\"Topic\" + str(i) for i in range(model.n_components)]\n",
    "    # index names\n",
    "    docnames = [\"Doc\" + str(i) for i in range(len(corpus))]\n",
    "    \n",
    "    # Make the pandas dataframe\n",
    "    df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "    # Get dominant topic for each document\n",
    "    dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "    df_document_topic['dominant_topic'] = dominant_topic   \n",
    "    df_document_topic = df_document_topic.reset_index()\n",
    "        \n",
    "    #combine all the search terms into one data frame\n",
    "    tweets_topics = tweets_search_topics.merge(df_document_topic, left_index=True, right_index=True, how='left')\n",
    "    tweets_topics_words = tweets_topics.merge(df_topic_keywords, how='left', left_on='dominant_topic', right_on='topic_index')\n",
    "    tweets_all_topics = tweets_all_topics.append(tweets_topics_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384, 28)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweets</th>\n",
       "      <th>permalink</th>\n",
       "      <th>date</th>\n",
       "      <th>formatted_date</th>\n",
       "      <th>favorites</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>...</th>\n",
       "      <th>Topic3</th>\n",
       "      <th>Topic4</th>\n",
       "      <th>dominant_topic</th>\n",
       "      <th>index_y</th>\n",
       "      <th>Word 0</th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "      <th>topic_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1177593827768422400</td>\n",
       "      <td>8.579987e+08</td>\n",
       "      <td>it is a free market so loans will exist. howev...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://twitter.com/liujackc/status/1177593827...</td>\n",
       "      <td>2019-09-27 14:40:21+00:00</td>\n",
       "      <td>Fri Sep 27 14:40:21 +0000 2019</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>Topic 1</td>\n",
       "      <td>income</td>\n",
       "      <td>buy</td>\n",
       "      <td>italian</td>\n",
       "      <td>caused</td>\n",
       "      <td>high</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1169176926977581057</td>\n",
       "      <td>2.211808e+07</td>\n",
       "      <td>lucky bastard! actually i know someone who did...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://twitter.com/Steve_Dunthorne/status/116...</td>\n",
       "      <td>2019-09-04 09:14:36+00:00</td>\n",
       "      <td>Wed Sep 04 09:14:36 +0000 2019</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0</td>\n",
       "      <td>Topic 0</td>\n",
       "      <td>china</td>\n",
       "      <td>feeling</td>\n",
       "      <td>think</td>\n",
       "      <td>accounted</td>\n",
       "      <td>cares</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1167796354032078850</td>\n",
       "      <td>2.566628e+09</td>\n",
       "      <td>football historians will look back in astonish...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://twitter.com/LysaghtSteph/status/116779...</td>\n",
       "      <td>2019-08-31 13:48:42+00:00</td>\n",
       "      <td>Sat Aug 31 13:48:42 +0000 2019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>Topic 3</td>\n",
       "      <td>pogba</td>\n",
       "      <td>man</td>\n",
       "      <td>chinese</td>\n",
       "      <td>soared</td>\n",
       "      <td>cantonese</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1164907704894099462</td>\n",
       "      <td>5.383800e+07</td>\n",
       "      <td>#pirata #bravo #italian best italian food with...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://twitter.com/raymoorecn/status/11649077...</td>\n",
       "      <td>2019-08-23 14:30:14+00:00</td>\n",
       "      <td>Fri Aug 23 14:30:14 +0000 2019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#pirata #bravo #italian</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1</td>\n",
       "      <td>Topic 1</td>\n",
       "      <td>income</td>\n",
       "      <td>buy</td>\n",
       "      <td>italian</td>\n",
       "      <td>caused</td>\n",
       "      <td>high</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1163704451305054208</td>\n",
       "      <td>2.162408e+09</td>\n",
       "      <td>most hk chinese were in that camp in 1997. hat...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://twitter.com/DrJim_Walker/status/116370...</td>\n",
       "      <td>2019-08-20 06:48:56+00:00</td>\n",
       "      <td>Tue Aug 20 06:48:56 +0000 2019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>Topic 3</td>\n",
       "      <td>pogba</td>\n",
       "      <td>man</td>\n",
       "      <td>chinese</td>\n",
       "      <td>soared</td>\n",
       "      <td>cantonese</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id     author_id  \\\n",
       "0  1177593827768422400  8.579987e+08   \n",
       "1  1169176926977581057  2.211808e+07   \n",
       "2  1167796354032078850  2.566628e+09   \n",
       "3  1164907704894099462  5.383800e+07   \n",
       "4  1163704451305054208  2.162408e+09   \n",
       "\n",
       "                                                text  retweets  \\\n",
       "0  it is a free market so loans will exist. howev...       0.0   \n",
       "1  lucky bastard! actually i know someone who did...       0.0   \n",
       "2  football historians will look back in astonish...       0.0   \n",
       "3  #pirata #bravo #italian best italian food with...       0.0   \n",
       "4  most hk chinese were in that camp in 1997. hat...       0.0   \n",
       "\n",
       "                                           permalink  \\\n",
       "0  https://twitter.com/liujackc/status/1177593827...   \n",
       "1  https://twitter.com/Steve_Dunthorne/status/116...   \n",
       "2  https://twitter.com/LysaghtSteph/status/116779...   \n",
       "3  https://twitter.com/raymoorecn/status/11649077...   \n",
       "4  https://twitter.com/DrJim_Walker/status/116370...   \n",
       "\n",
       "                        date                  formatted_date  favorites  \\\n",
       "0  2019-09-27 14:40:21+00:00  Fri Sep 27 14:40:21 +0000 2019        3.0   \n",
       "1  2019-09-04 09:14:36+00:00  Wed Sep 04 09:14:36 +0000 2019        2.0   \n",
       "2  2019-08-31 13:48:42+00:00  Sat Aug 31 13:48:42 +0000 2019        0.0   \n",
       "3  2019-08-23 14:30:14+00:00  Fri Aug 23 14:30:14 +0000 2019        0.0   \n",
       "4  2019-08-20 06:48:56+00:00  Tue Aug 20 06:48:56 +0000 2019        0.0   \n",
       "\n",
       "  mentions                 hashtags  ...  Topic3 Topic4 dominant_topic  \\\n",
       "0      NaN                      NaN  ...    0.01   0.01              1   \n",
       "1      NaN                      NaN  ...    0.02   0.02              0   \n",
       "2      NaN                      NaN  ...    0.96   0.01              3   \n",
       "3      NaN  #pirata #bravo #italian  ...    0.02   0.02              1   \n",
       "4      NaN                      NaN  ...    0.96   0.01              3   \n",
       "\n",
       "   index_y  Word 0   Word 1   Word 2     Word 3     Word 4  topic_index  \n",
       "0  Topic 1  income      buy  italian     caused       high            1  \n",
       "1  Topic 0   china  feeling    think  accounted      cares            0  \n",
       "2  Topic 3   pogba      man  chinese     soared  cantonese            3  \n",
       "3  Topic 1  income      buy  italian     caused       high            1  \n",
       "4  Topic 3   pogba      man  chinese     soared  cantonese            3  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_all_topics = tweets_all_topics.reset_index(drop=True)\n",
    "print(tweets_all_topics.shape)\n",
    "tweets_all_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_all_topics.to_csv(r'C:\\Users\\USER\\Desktop\\twitter-sentiment-stock-prediction\\processed_data\\tweets_all_topics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis using Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import h5py\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, 56, 300)           120000300 \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 256)               439296    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 120,576,310\n",
      "Trainable params: 576,010\n",
      "Non-trainable params: 120,000,300\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Unterminated string starting at: line 1 column 1770476 (char 1770475)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-e50271e6065b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprd_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprd_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mword_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\USER\\Desktop\\twitter-sentiment-stock-prediction\\models\\dl_sentiment_model\\word_idx.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n\u001b[0m\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 348\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \"\"\"\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    351\u001b[0m         \"\"\"\n\u001b[0;32m    352\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Unterminated string starting at: line 1 column 1770476 (char 1770475)"
     ]
    }
   ],
   "source": [
    "# read in the weight of the trained model.\n",
    "weight_path = r'C:\\Users\\USER\\Desktop\\twitter-sentiment-stock-prediction\\models\\dl_sentiment_model\\best_weight_glove_bi_512.hdf5'\n",
    "prd_model = load_model(weight_path)\n",
    "prd_model.summary()\n",
    "word_idx = json.load(open(r\"C:\\Users\\USER\\Desktop\\twitter-sentiment-stock-prediction\\models\\dl_sentiment_model\\word_idx.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_DL(prd_model, text_data, word_idx):\n",
    "\n",
    "    #data = \"Pass the salt\"\n",
    "\n",
    "    live_list = []\n",
    "    batchSize = len(text_data)\n",
    "    live_list_np = np.zeros((56,batchSize))\n",
    "    for index, row in text_data.iterrows():\n",
    "        #print (index)\n",
    "        text_data_sample = text_data['text'][index]\n",
    "        # split the sentence into its words and remove any punctuations.\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        text_data_list = tokenizer.tokenize(text_data_sample)\n",
    "\n",
    "        #text_data_list = text_data_sample.split()\n",
    "\n",
    "\n",
    "        labels = np.array(['1','2','3','4','5','6','7','8','9','10'], dtype = \"int\")\n",
    "        #word_idx['I']\n",
    "        # get index for the live stage\n",
    "        data_index = np.array([word_idx[word.lower()] if word.lower() in word_idx else 0 for word in text_data_list])\n",
    "        data_index_np = np.array(data_index)\n",
    "\n",
    "        # padded with zeros of length 56 i.e maximum length\n",
    "        padded_array = np.zeros(56)\n",
    "        padded_array[:data_index_np.shape[0]] = data_index_np[:56]\n",
    "        data_index_np_pad = padded_array.astype(int)\n",
    "\n",
    "\n",
    "        live_list.append(data_index_np_pad)\n",
    "\n",
    "    live_list_np = np.asarray(live_list)\n",
    "    score = prd_model.predict(live_list_np, batch_size=batchSize, verbose=0)\n",
    "    single_score = np.round(np.dot(score, labels)/10,decimals=2)\n",
    "\n",
    "    score_all  = []\n",
    "    for each_score in score:\n",
    "\n",
    "        top_3_index = np.argsort(each_score)[-3:]\n",
    "        top_3_scores = each_score[top_3_index]\n",
    "        top_3_weights = top_3_scores/np.sum(top_3_scores)\n",
    "        single_score_dot = np.round(np.dot(top_3_index, top_3_weights)/10, decimals = 2)\n",
    "        score_all.append(single_score_dot)\n",
    "\n",
    "    text_data['Sentiment_Score'] = pd.DataFrame(score_all)\n",
    "\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-22d1cf64afcd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtext_data\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mtweets_all_topics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Deep Learning sentiment scoring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtext_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_sentiment_DL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprd_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'word_idx' is not defined"
     ]
    }
   ],
   "source": [
    "text_data =  tweets_all_topics\n",
    "# Deep Learning sentiment scoring\n",
    "text_out = get_sentiment_DL(prd_model, text_data, word_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### example of negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_out.sort_values(by='Sentiment_Score')[['text','Sentiment_Score']].head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  example of positive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_out.sort_values(by='Sentiment_Score', ascending=False)[['text','Sentiment_Score']].head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_out.to_csv('C:\\Users\\USER\\Desktop\\twitter-sentiment-stock-prediction\\processed_data\\tweets_topics_sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
